import io
import json
from typing import Dict, Any

from PIL import Image

from ..config import Settings
from ..config.logging import get_logger
from ..database.repository import TaskRepository
from ..ml.factory import create_inference_service
from ..ml.inference_interface import ModelInferenceInterface
from ..storage.s3_client import S3StorageService

logger = get_logger(__name__)


class TaskProcessor:
    """
    –ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π.

    –ù–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π ML-–º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é (Dependency Injection + Interface).
    """

    def __init__(
            self,
            repository: TaskRepository,
            storage: S3StorageService,
            settings: Settings
    ):
        self.repository = repository
        self.storage = storage
        self.settings = settings

        # üî• –ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢: –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É
        # –í –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ml_service –±—É–¥–µ—Ç –ª–∏–±–æ Mock, –ª–∏–±–æ TripoSR (–∫–æ–≥–¥–∞ –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤)
        self.ml_service: ModelInferenceInterface = create_inference_service(settings)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        if not self.ml_service.is_available():
            logger.error("ML Service failed to initialize!")

    def process(self, task_data: Dict[str, Any]) -> bool:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        :param task_data: –î–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–∑ Kafka
        :return: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        task_id = task_data.get("task_id", "unknown")
        input_config = task_data.get("input", {})
        params = task_data.get("params", {})

        input_s3_path = input_config.get("s3_path")
        output_format = params.get("output_format", "obj")

        logger.info(f"üöÄ Starting task {task_id} for {input_s3_path}")

        try:
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            self.repository.update_status(task_id, 'PROCESSING')

            # 2. –°–∫–∞—á–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ S3
            logger.info(f"üì• Downloading image from {input_s3_path}")
            image_bytes = self.storage.download_file(input_s3_path)
            image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

            # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (—á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å!)
            # Processor –Ω–µ –∑–Ω–∞–µ—Ç, —á—Ç–æ –≤–Ω—É—Ç—Ä–∏: —Å–æ–Ω –∏–ª–∏ GPU
            logger.info("üß† Running ML inference...")
            result = self.ml_service.infer(image, params)

            if not result.success:
                raise RuntimeError(f"ML Inference failed: {result.error}")

            # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ S3
            mesh_key = f"results/{task_id}/model.{output_format}"
            texture_key = f"results/{task_id}/texture.png"

            mesh_url = self.storage.upload_bytes(mesh_key, result.mesh_bytes)

            texture_url = None
            if result.texture_bytes:
                texture_url = self.storage.upload_bytes(texture_key, result.texture_bytes)

            # 5. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î
            db_result = {
                "model_url": mesh_url,
                "texture_url": texture_url,
                "output_format": output_format,
                "metrics": result.metrics
            }

            # 6. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å + —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.repository.update_status(task_id, 'COMPLETED', json.dumps(db_result))

            logger.info(f"‚úÖ Task {task_id} completed. Model: {mesh_url}")
            return True

        except Exception as e:
            logger.error(f"Error processing task {task_id}: {e}", exc_info=True)
            self.repository.update_status(task_id, 'FAILED')
            return False
